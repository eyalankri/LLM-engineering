{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import threading\n",
    "import gradio as gr\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91665cc9-49ff-48db-9d79-09389ea589a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_open_ai():\n",
    "    load_dotenv(override=True)\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    if not api_key and api_key.startswith('sk-proj-') and len(api_key) > 10:\n",
    "        print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "            \n",
    "    openai = OpenAI()\n",
    "    MODEL_GPT = 'gpt-4o-mini'\n",
    " \n",
    "    return openai, MODEL_GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "143eac33-5c75-4b59-8ebb-09d39189f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ollama():\n",
    "    OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "    MODEL = \"llama3.2\"\n",
    "    \n",
    "    return OLLAMA_API, HEADERS, MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You will be given a code snippet. \"\n",
    "    \"Your task is to explain what the code does and why it was written that way, using clear and simple language suitable for beginner developers. \"\n",
    "    \"Provide a step-by-step explanation and highlight any best practices or potential issues. \"\n",
    "    \"You are an experienced and thoughtful tech lead who enjoys mentoring others. \"\n",
    "    \"Do not introduce yourself or describe who you are in the response.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4202b4aa-5bb2-413a-b37e-6ce42f1ff82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompts(code):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Please explain what this code does and why: \" + code}\n",
    "    ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9525fb0-47b6-4208-82bb-21e4b2095808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_code_open_ai(code):\n",
    "    openai, model = load_open_ai()\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=build_prompts(code),\n",
    "            stream=False\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"**OpenAI API Error:** {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5100b8f2-6343-4e86-8d3e-8a16e93842ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_code_open_ollama(code):  \n",
    "    api, headers, model = load_ollama()\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": build_prompts(code),\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(api, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # convert Response to dict\n",
    "        content = data[\"message\"][\"content\"]  # extract content\n",
    "        return content\n",
    "        # display(Markdown(f\"### üêë I am **{model.upper()}**\\n\\n\" + content))\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ef6f739-b17d-4df1-b2fb-9410bd02ab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "Could not create share link. Missing file: C:\\Users\\eyala\\.cache\\huggingface\\gradio\\frpc\\frpc_windows_amd64_v0.3. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_windows_amd64.exe\n",
      "2. Rename the downloaded file to: frpc_windows_amd64_v0.3\n",
      "3. Move the file to this location: C:\\Users\\eyala\\.cache\\huggingface\\gradio\\frpc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as view:\n",
    "    model_choice = gr.Radio([\"OpenAI\", \"Ollama\"], label=\"Select Model\", value=\"OpenAI\")\n",
    "    input_box = gr.Textbox(label=\"Your message:\")\n",
    "    submit_btn = gr.Button(\"Submit\")\n",
    "    output_box = gr.Markdown()\n",
    "    \n",
    "    def process_with_loading(message, model):\n",
    "        if model == \"OpenAI\":\n",
    "            return explain_code_open_ai(message)\n",
    "        else:  # Ollama\n",
    "            return explain_code_open_ollama(message)\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=process_with_loading,\n",
    "        inputs=[input_box, model_choice],\n",
    "        outputs=[output_box],\n",
    "    ).then(\n",
    "        lambda: gr.Button(\"Submit\", interactive=True),\n",
    "        outputs=[submit_btn]\n",
    "    )\n",
    "    \n",
    "    # This will show the button as \"Processing...\" while processing\n",
    "    submit_btn.click(\n",
    "        lambda: gr.Button(\"Processing...\", interactive=False),\n",
    "        outputs=[submit_btn]\n",
    "    )\n",
    "\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
